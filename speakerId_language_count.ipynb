{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unique speaker in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def get_unique_speaker_ids(folder_path):\n",
    "    # List all files in the folder\n",
    "    file_list = os.listdir(folder_path)\n",
    "    #print(file_list[0])\n",
    "    # Set to store unique speaker IDs\n",
    "    unique_speaker_ids = set()\n",
    "    \n",
    "    # Iterate over each file\n",
    "    for file_name in file_list:\n",
    "        # Extract speaker ID from the filename\n",
    "        if file_name.endswith('.wav'):\n",
    "            speaker_id = file_name.split('_')[5]\n",
    "            \n",
    "            # Add speaker ID to the set\n",
    "            unique_speaker_ids.add(speaker_id)\n",
    "    \n",
    "    return unique_speaker_ids\n",
    "\n",
    "def write_unique_speaker_ids_to_csv(unique_speaker_ids, output_csv_file):\n",
    "    with open(output_csv_file, 'w', newline='') as csvfile: \n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        for speaker_id in unique_speaker_ids:\n",
    "            csv_writer.writerow([speaker_id])\n",
    "\n",
    "# Parent folder containing multiple subfolders with .wav files\n",
    "parent_folder = '/raid/scratch/Vaibhav/Dataset/Audio_language_specific_part2'\n",
    "\n",
    "# Output CSV file to store combined unique speaker IDs\n",
    "combined_csv_file = '/raid/scratch/Vaibhav/Dataset/combined_unique_speaker_ids.csv'\n",
    "\n",
    "# Initialize an empty set to store all unique speaker IDs\n",
    "all_unique_speaker_ids = set()\n",
    "\n",
    "# Iterate over each subfolder\n",
    "for folder in os.listdir(parent_folder):\n",
    "    print(folder)\n",
    "    folder_path = os.path.join(parent_folder, folder)\n",
    "\n",
    "    for subfolder in os.listdir(folder_path):\n",
    "\n",
    "        print(subfolder)\n",
    "        subfolder_path = os.path.join(folder_path, subfolder)\n",
    "        #print(subfolder_path)\n",
    "        # Get unique speaker IDs for the current subfolder\n",
    "        unique_speaker_ids = get_unique_speaker_ids(subfolder_path)\n",
    "\n",
    "        # Add unique speaker IDs to the set of all unique speaker IDs\n",
    "        all_unique_speaker_ids.update(unique_speaker_ids)\n",
    "\n",
    "# Write all unique speaker IDs to the combined CSV file\n",
    "write_unique_speaker_ids_to_csv(all_unique_speaker_ids, combined_csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  count of lanuages one speaker can speak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_speaker_id_from_filename(filename):\n",
    "    # Extract speaker ID from the filename\n",
    "    if filename.endswith('.wav'):\n",
    "        return filename.split('_')[5]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def count_folders_with_speaker_id(parent_folder, speaker_id):\n",
    "    count = 0\n",
    "    folders = []\n",
    "    \n",
    "    # Iterate over each subfolder\n",
    "    for folder in os.listdir(parent_folder):\n",
    "        folder_path = os.path.join(parent_folder, folder)\n",
    "\n",
    "        for subfolder in os.listdir(folder_path):\n",
    "            subfolder_path = os.path.join(folder_path, subfolder)\n",
    "            #print(subfolder)\n",
    "\n",
    "            # Check if the folder contains .wav files with the specified speaker ID\n",
    "            if os.path.isdir(subfolder_path):\n",
    "                for file_name in os.listdir(subfolder_path):\n",
    "                    if get_speaker_id_from_filename(file_name) == speaker_id:\n",
    "                        folders.append(subfolder)\n",
    "                        count += 1\n",
    "                        print(folders, count)\n",
    "                        break  # No need to check other files in this folder\n",
    "        \n",
    "    return count, folders\n",
    "\n",
    "# Parent folder containing multiple subfolders with .wav files\n",
    "parent_folder = '/raid/scratch/Vaibhav/Dataset/Audio_language_specific_part2'\n",
    "\n",
    "# CSV file containing all unique speaker IDs\n",
    "unique_speaker_ids_csv = '/raid/scratch/Vaibhav/Dataset/combined_unique_speaker_ids.csv'\n",
    "\n",
    "# Output CSV file to store speaker IDs, their counts, and corresponding folder names\n",
    "output_csv_file = '/raid/scratch/Vaibhav/Dataset/speaker_id_counts_with_lanuages.csv'\n",
    "\n",
    "# Read the CSV file containing all unique speaker IDs\n",
    "all_unique_speaker_ids = []\n",
    "with open(unique_speaker_ids_csv, 'r') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    for row in csv_reader:\n",
    "        all_unique_speaker_ids.append(row[0])\n",
    "\n",
    "# Create a list to store speaker ID counts, number of folders, and corresponding folder names\n",
    "speaker_id_counts_with_folders = []\n",
    "\n",
    "# Create a tqdm progress bar for all unique speaker IDs\n",
    "pbar = tqdm(total=len(all_unique_speaker_ids), desc=\"Processing Speaker IDs\", unit=\"ID\")\n",
    "\n",
    "count_speaker = 0\n",
    "\n",
    "# Count the number of folders containing each speaker ID and get the folder names\n",
    "for speaker_id in all_unique_speaker_ids:\n",
    "    count_speaker += 1\n",
    "\n",
    "    count, folders = count_folders_with_speaker_id(parent_folder, speaker_id)\n",
    "    speaker_id_counts_with_folders.append([speaker_id, count, ', '.join(folders)])\n",
    "\n",
    "    if count_speaker >= 10:\n",
    "        break\n",
    "    \n",
    "    # Update the progress bar\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Write speaker ID counts, number of folders, and folder names to the output CSV file\n",
    "with open(output_csv_file, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Speaker ID', 'Number of Folders', 'Folders'])\n",
    "    csv_writer.writerows(speaker_id_counts_with_folders)\n",
    "\n",
    "print(\"Speaker ID counts, number of folders, and corresponding folder names have been written to:\", output_csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure out in how many cases same speaker's file get detected with different languages and to what extent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Input folder containing CSV files\n",
    "input_folder = \"/data/Vaani/CSVs_with_link/\"\n",
    "\n",
    "# Output CSV file\n",
    "output_csv_file = \"speakerid_true_pred_different1.csv\"\n",
    "\n",
    "# Dictionary to store aggregated data for each speaker ID\n",
    "speaker_data = {}\n",
    "\n",
    "for folder in tqdm(os.listdir(input_folder)):\n",
    "    folder_path = os.path.join(input_folder, folder)\n",
    "\n",
    "    # Use tqdm to create a progress bar for CSV files within the folder\n",
    "    for filename in tqdm(os.listdir(folder_path)):\n",
    "        #print(filename)\n",
    "\n",
    "        if filename.endswith(\".csv\"):\n",
    "            input_csv_file = os.path.join(folder_path, filename)\n",
    "\n",
    "            with open(input_csv_file, mode='r') as file:\n",
    "                reader = csv.DictReader(file)\n",
    "\n",
    "                for row in reader:\n",
    "                    speaker_id = os.path.basename(row['File']).split('_')[5]\n",
    "                    district = os.path.basename(row['File']).split('_')[4]\n",
    "                    state = os.path.basename(row['File']).split('_')[3]\n",
    "\n",
    "                    # Update speaker data if speaker_id doesn't exist\n",
    "                    if speaker_id not in speaker_data:\n",
    "                        speaker_data[speaker_id] = {\n",
    "                            'Vendors': set(),\n",
    "                            'Languages': {\n",
    "                                'Asserted': {},\n",
    "                                'Predicted': {}\n",
    "                            },\n",
    "                            'District': district,\n",
    "                            'State': state\n",
    "                        }\n",
    "\n",
    "                    speaker_data[speaker_id]['Vendors'].add(row['Vendor'])\n",
    "\n",
    "                    # Count asserted languages\n",
    "                    asserted_language = row['Asserted_Language']\n",
    "                    if asserted_language not in speaker_data[speaker_id]['Languages']['Asserted']:\n",
    "                        speaker_data[speaker_id]['Languages']['Asserted'][asserted_language] = 0\n",
    "                    speaker_data[speaker_id]['Languages']['Asserted'][asserted_language] += 1\n",
    "\n",
    "                    # Count predicted languages\n",
    "                    predicted_language = row['Detected_Language']\n",
    "                    if predicted_language not in speaker_data[speaker_id]['Languages']['Predicted']:\n",
    "                        speaker_data[speaker_id]['Languages']['Predicted'][predicted_language] = 0\n",
    "\n",
    "                    speaker_data[speaker_id]['Languages']['Predicted'][predicted_language] += 1\n",
    "\n",
    "\n",
    "# Write aggregated data to output CSV file\n",
    "with open(output_csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['SpeakerID', 'Vendors', 'Asserted_Languages', 'Asserted_Languages_Count', 'Predicted_Languages', 'Predicted_Languages_Count', 'District', 'State'])\n",
    "    for speaker_id, data in speaker_data.items():\n",
    "        vendors = ', '.join(data['Vendors'])\n",
    "        asserted_languages = ', '.join(data['Languages']['Asserted'].keys())\n",
    "        asserted_languages_count = ', '.join(str(count) for count in data['Languages']['Asserted'].values())\n",
    "        predicted_languages = ', '.join(data['Languages']['Predicted'].keys())\n",
    "        predicted_languages_count = ', '.join(str(count) for count in data['Languages']['Predicted'].values())\n",
    "        writer.writerow([speaker_id, vendors, asserted_languages, asserted_languages_count, predicted_languages, predicted_languages_count, data['District'], data['State']])\n",
    "\n",
    "print(\"Aggregated data has been written to\", output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of occurrences with 1 predicted labels for a speaker 5326\n",
      "Number of occurrences with 2 predicted labels for a speaker 7845\n",
      "Number of occurrences with 3 predicted labels for a speaker 7799\n",
      "Number of occurrences with 4 predicted labels for a speaker 6918\n",
      "Number of occurrences with 5 predicted labels for a speaker 5687\n",
      "Number of occurrences with 6 predicted labels for a speaker 4606\n",
      "Number of occurrences with 7 predicted labels for a speaker 3991\n",
      "Number of occurrences with 8 predicted labels for a speaker 3328\n",
      "Number of occurrences with 9 predicted labels for a speaker 2826\n",
      "Number of occurrences with 10 predicted labels for a speaker 2282\n",
      "Number of occurrences with 11 predicted labels for a speaker 1921\n",
      "Number of occurrences with 12 predicted labels for a speaker 1750\n",
      "Number of occurrences with 13 predicted labels for a speaker 1507\n",
      "Number of occurrences with 14 predicted labels for a speaker 1302\n",
      "Number of occurrences with 15 predicted labels for a speaker 1132\n",
      "Number of occurrences with 16 predicted labels for a speaker 970\n",
      "Number of occurrences with 17 predicted labels for a speaker 907\n",
      "Number of occurrences with 18 predicted labels for a speaker 726\n",
      "Number of occurrences with 19 predicted labels for a speaker 664\n",
      "Number of occurrences with 20 predicted labels for a speaker 619\n",
      "Number of occurrences with 21 predicted labels for a speaker 508\n",
      "Number of occurrences with 22 predicted labels for a speaker 422\n",
      "Number of occurrences with 23 predicted labels for a speaker 359\n",
      "Number of occurrences with 24 predicted labels for a speaker 319\n",
      "Number of occurrences with 25 predicted labels for a speaker 304\n",
      "Number of occurrences with 26 predicted labels for a speaker 248\n",
      "Number of occurrences with 27 predicted labels for a speaker 220\n",
      "Number of occurrences with 28 predicted labels for a speaker 213\n",
      "Number of occurrences with 29 predicted labels for a speaker 184\n",
      "Number of occurrences with 30 predicted labels for a speaker 190\n",
      "Number of occurrences with 31 predicted labels for a speaker 154\n",
      "Number of occurrences with 32 predicted labels for a speaker 139\n",
      "Number of occurrences with 33 predicted labels for a speaker 136\n",
      "Number of occurrences with 34 predicted labels for a speaker 107\n",
      "Number of occurrences with 35 predicted labels for a speaker 82\n",
      "Number of occurrences with 36 predicted labels for a speaker 113\n",
      "Number of occurrences with 37 predicted labels for a speaker 81\n",
      "Number of occurrences with 38 predicted labels for a speaker 77\n",
      "Number of occurrences with 39 predicted labels for a speaker 75\n",
      "Number of occurrences with 40 predicted labels for a speaker 65\n",
      "Number of occurrences with 41 predicted labels for a speaker 68\n",
      "Number of occurrences with 42 predicted labels for a speaker 50\n",
      "Number of occurrences with 43 predicted labels for a speaker 39\n",
      "Number of occurrences with 44 predicted labels for a speaker 42\n",
      "Number of occurrences with 45 predicted labels for a speaker 39\n",
      "Number of occurrences with 46 predicted labels for a speaker 34\n",
      "Number of occurrences with 47 predicted labels for a speaker 35\n",
      "Number of occurrences with 48 predicted labels for a speaker 32\n",
      "Number of occurrences with 49 predicted labels for a speaker 26\n",
      "Number of occurrences with 50 predicted labels for a speaker 21\n",
      "Number of occurrences with 51 predicted labels for a speaker 21\n",
      "Number of occurrences with 52 predicted labels for a speaker 17\n",
      "Number of occurrences with 53 predicted labels for a speaker 16\n",
      "Number of occurrences with 54 predicted labels for a speaker 22\n",
      "Number of occurrences with 55 predicted labels for a speaker 15\n",
      "Number of occurrences with 56 predicted labels for a speaker 10\n",
      "Number of occurrences with 57 predicted labels for a speaker 9\n",
      "Number of occurrences with 58 predicted labels for a speaker 10\n",
      "Number of occurrences with 59 predicted labels for a speaker 11\n",
      "Number of occurrences with 60 predicted labels for a speaker 5:['Moni12351', 'Visw68301', 'Anup00412', 'Parv75380', 'Arij09805']\n",
      "Number of occurrences with 61 predicted labels for a speaker 6\n",
      "Number of occurrences with 62 predicted labels for a speaker 6\n",
      "Number of occurrences with 63 predicted labels for a speaker 9\n",
      "Number of occurrences with 64 predicted labels for a speaker 8\n",
      "Number of occurrences with 65 predicted labels for a speaker 6\n",
      "Number of occurrences with 66 predicted labels for a speaker 4:['Mani69393', 'Sikh71828', 'Kazi21534', 'Paya24512']\n",
      "Number of occurrences with 67 predicted labels for a speaker 5:['Tejp97157', 'Chan10964', 'Aayu53842', 'Mahe43242', 'Ajay63524']\n",
      "Number of occurrences with 68 predicted labels for a speaker 3:['30011287', 'Khoo71796', 'Muke42900']\n",
      "Number of occurrences with 69 predicted labels for a speaker 3:['Anur86358', 'Mahe44591', 'Rame05277']\n",
      "Number of occurrences with 70 predicted labels for a speaker 4:['Ajee37870', 'Maya24510', 'Ranj00987', 'Chun36255']\n",
      "Number of occurrences with 71 predicted labels for a speaker 3:['Sanj44304', '00629979', 'Gudd22153']\n",
      "Number of occurrences with 73 predicted labels for a speaker 5:['Mahi99471', 'Jagd32589', '73091350', 'Balr96200', '60775524']\n",
      "Number of occurrences with 74 predicted labels for a speaker 1:['Prav86046']\n",
      "Number of occurrences with 75 predicted labels for a speaker 3:['Dind62567', 'Rake44560', 'Kavi70873']\n",
      "Number of occurrences with 77 predicted labels for a speaker 2:['Mosm12310', 'Mina02966']\n",
      "Number of occurrences with 78 predicted labels for a speaker 1:['58547249']\n",
      "Number of occurrences with 79 predicted labels for a speaker 1:['Vika44521']\n",
      "Number of occurrences with 81 predicted labels for a speaker 1:['Sura08129']\n",
      "Number of occurrences with 82 predicted labels for a speaker 1:['Mant188589']\n",
      "Number of occurrences with 86 predicted labels for a speaker 1:['Sanj87845']\n",
      "Number of occurrences with 87 predicted labels for a speaker 1:['Ajay44593']\n",
      "Number of occurrences with 89 predicted labels for a speaker 1:['Bish11648']\n",
      "Number of occurrences with 90 predicted labels for a speaker 1:['Akas90627']\n",
      "Number of occurrences with 97 predicted labels for a speaker 2:['Anu10503', 'Monu51789']\n",
      "Maximum no. of predicted labels: 97\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Input CSV file\n",
    "input_csv_file = \"/data/Vaani/speakerid_true_pred_different3.csv\"\n",
    "\n",
    "# Dictionary to store counts for different lengths of predicted labels\n",
    "label_counts = {i: [] for i in range(1, 100)}\n",
    "\n",
    "# Variable to store the maximum length of predicted labels\n",
    "max_length = 0\n",
    "\n",
    "# Read data from input CSV file\n",
    "with open(input_csv_file, mode='r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        predicted_languages = row['Predicted_Languages'].split(', ')\n",
    "        num_predicted_languages = len(predicted_languages)\n",
    "        \n",
    "        # Increment count based on the length of predicted labels\n",
    "        label_counts[num_predicted_languages].append(row['SpeakerID'])\n",
    "        \n",
    "        # Update the maximum length\n",
    "        if num_predicted_languages > max_length:\n",
    "            max_length = num_predicted_languages\n",
    "\n",
    "# Print the counts for different lengths of predicted labels along with speaker IDs\n",
    "for length, speaker_ids in label_counts.items():\n",
    "    if len(speaker_ids) != 0:\n",
    "        if len(speaker_ids) <= 5:\n",
    "            print(f\"Number of occurrences with {length} predicted labels for a speaker {len(speaker_ids)}:{speaker_ids}\")\n",
    "        else :\n",
    "            print(f\"Number of occurrences with {length} predicted labels for a speaker {len(speaker_ids)}\")\n",
    "\n",
    "\n",
    "# Print the maximum length of predicted labels\n",
    "print(f\"Maximum no. of predicted labels: {max_length}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# added the probability column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSVs: 100%|██████████| 23/23 [00:03<00:00,  7.54it/s]\n",
      "Processing CSVs: 100%|██████████| 36/36 [01:19<00:00,  2.21s/it]\n",
      "Processing: 100%|██████████| 2/2 [01:22<00:00, 41.29s/it]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Input folder containing CSV files\n",
    "input_folder = \"/data/Vaani/CSVs_with_link/\"\n",
    "\n",
    "# Output CSV file\n",
    "output_csv_file = \"speakerid_true_pred_different3.csv\"\n",
    "\n",
    "# Dictionary to store aggregated data for each speaker ID\n",
    "speaker_data = {}\n",
    "\n",
    "\n",
    "def process_csv_file(input_csv_file):\n",
    "    \"\"\"\n",
    "    Processes a single CSV file, extracting speaker data and updating the dictionary.\n",
    "\n",
    "    Args:\n",
    "        input_csv_file (str): Path to the CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(input_csv_file, mode='r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "\n",
    "        for row in reader:\n",
    "            speaker_id = os.path.basename(row['File']).split('_')[5]\n",
    "            district = os.path.basename(row['File']).split('_')[4]\n",
    "            state = os.path.basename(row['File']).split('_')[3]\n",
    "\n",
    "            # Update speaker data if speaker_id doesn't exist\n",
    "            if speaker_id not in speaker_data:\n",
    "                speaker_data[speaker_id] = {\n",
    "                    'Vendors': set(),\n",
    "                    'Languages': {\n",
    "                        'Asserted': {},\n",
    "                        'Predicted': {}\n",
    "                    },\n",
    "                    'District': district,\n",
    "                    'State': state\n",
    "                }\n",
    "\n",
    "            speaker_data[speaker_id]['Vendors'].add(row['Vendor'])\n",
    "\n",
    "            # Count asserted languages\n",
    "            asserted_language = row['Asserted_Language']\n",
    "            if asserted_language not in speaker_data[speaker_id]['Languages']['Asserted']:\n",
    "                speaker_data[speaker_id]['Languages']['Asserted'][asserted_language] = 0\n",
    "            speaker_data[speaker_id]['Languages']['Asserted'][asserted_language] += 1\n",
    "\n",
    "            # Count predicted languages and probabilities (assuming a 'Probability' column)\n",
    "            predicted_language = row['Detected_Language']\n",
    "            probability = float(row['Probability'])  # Assuming 'Probability' column exists\n",
    "            probability = round(probability, 2)\n",
    "\n",
    "            if predicted_language not in speaker_data[speaker_id]['Languages']['Predicted']:\n",
    "                speaker_data[speaker_id]['Languages']['Predicted'][predicted_language] = {\n",
    "                    'count': 0,\n",
    "                    'probability': []\n",
    "                }\n",
    "            speaker_data[speaker_id]['Languages']['Predicted'][predicted_language]['count'] += 1\n",
    "            speaker_data[speaker_id]['Languages']['Predicted'][predicted_language]['probability'].append(probability)\n",
    "\n",
    "            # Calculate the mean probability if there are multiple probabilities\n",
    "            if len(speaker_data[speaker_id]['Languages']['Predicted'][predicted_language]['probability']) > 1:\n",
    "                mean_probability = sum(speaker_data[speaker_id]['Languages']['Predicted'][predicted_language]['probability']) / len(speaker_data[speaker_id]['Languages']['Predicted'][predicted_language]['probability'])\n",
    "                # Round the mean probability to two decimal places\n",
    "                mean_probability = round(mean_probability, 2)\n",
    "                # Replace the list of probabilities with the mean probability\n",
    "                speaker_data[speaker_id]['Languages']['Predicted'][predicted_language]['probability'] = [mean_probability]\n",
    "\n",
    "# Use a single tqdm loop for progress tracking\n",
    "for folder in tqdm(os.listdir(input_folder), desc=\"Processing\"):\n",
    "    folder_path = os.path.join(input_folder, folder)\n",
    "\n",
    "    for filename in tqdm(os.listdir(folder_path), desc=\"Processing CSVs\"):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            input_csv_file = os.path.join(folder_path, filename)\n",
    "            process_csv_file(input_csv_file)\n",
    "\n",
    "\n",
    "# Write aggregated data to output CSV file\n",
    "with open(output_csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['SpeakerID', 'Vendors', 'Asserted_Language', 'Files_count',\n",
    "                     'Predicted_Languages', 'Predicted_Language_Counts', 'Predicted_Language_Probabilities_mean',\n",
    "                     'District', 'State'])\n",
    "    for speaker_id, data in speaker_data.items():\n",
    "        vendors = ', '.join(data['Vendors'])\n",
    "        asserted_languages = ', '.join(data['Languages']['Asserted'].keys())\n",
    "        asserted_languages_count = ', '.join(str(count) for count in data['Languages']['Asserted'].values())\n",
    "\n",
    "        predicted_languages = []\n",
    "        predicted_language_counts = []\n",
    "        predicted_language_probabilities = []\n",
    "        for language, info in data['Languages']['Predicted'].items():\n",
    "            predicted_languages.append(language)\n",
    "            predicted_language_counts.append(str(info['count']))\n",
    "            predicted_language_probabilities.append(', '.join([str(prob) for prob in info['probability']]))\n",
    "\n",
    "        predicted_languages = ', '.join(predicted_languages)\n",
    "        predicted_language_counts = ', '.join(predicted_language_counts)\n",
    "        predicted_language_probabilities = ', '.join(predicted_language_probabilities)\n",
    "\n",
    "        writer.writerow([speaker_id, vendors, asserted_languages, asserted_languages_count,\n",
    "                         predicted_languages, predicted_language_counts, predicted_language_probabilities,\n",
    "                         data['District'], data['State']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making the csv file which contains the info assert lanuage wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data written to combined_output.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "# Function to extract state and district from filename\n",
    "def extract_state_and_district(filename):\n",
    "    parts = filename.split(\"_\")\n",
    "    if len(parts) >= 5:\n",
    "        state = parts[3]\n",
    "        district = parts[4]\n",
    "        return state, district\n",
    "    else:\n",
    "        return '', ''\n",
    "\n",
    "# Initialize dictionary to store combined data\n",
    "combined_data = defaultdict(lambda: {'vendors': set(), 'file_count': 0, 'predicted_language_count': defaultdict(int), 'predicted_language_probabilities': defaultdict(list), 'districts': set(), 'states': set()})\n",
    "\n",
    "# Directory containing CSV files\n",
    "input_directory = \"/data/Vaani/CSVs/CSVs_with_link/Not_supported_facebook\"\n",
    "\n",
    "# Process each CSV file in the directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        with open(os.path.join(input_directory, filename), \"r\") as input_file:\n",
    "            csv_reader = csv.DictReader(input_file)\n",
    "\n",
    "            for row in csv_reader:\n",
    "                asserted_language = row['Asserted_Language']\n",
    "                vendor = row['Vendor']\n",
    "                detected_language = row['Detected_Language']\n",
    "                probability = float(row['Probability'])\n",
    "                file_url = row['File']\n",
    "\n",
    "                # Extract filename from file URL\n",
    "                file_name = file_url.split(\"/\")[-1]\n",
    "\n",
    "                state, district = extract_state_and_district(file_name)\n",
    "                combined_data[asserted_language]['vendors'].add(vendor)\n",
    "                combined_data[asserted_language]['file_count'] += 1\n",
    "                combined_data[asserted_language]['predicted_language_count'][detected_language] += 1\n",
    "                combined_data[asserted_language]['predicted_language_probabilities'][detected_language].append(probability)\n",
    "                combined_data[asserted_language]['districts'].add(district)\n",
    "                combined_data[asserted_language]['states'].add(state)\n",
    "\n",
    "# Calculate the mean probability for each predicted language\n",
    "for asserted_language, data in combined_data.items():\n",
    "    for detected_language, probabilities in data['predicted_language_probabilities'].items():\n",
    "        mean_probability = sum(probabilities) / len(probabilities)\n",
    "        combined_data[asserted_language]['predicted_language_probabilities'][detected_language] = mean_probability\n",
    "\n",
    "# Write the combined data to a new CSV file\n",
    "output_file_path = \"combined_output.csv\"\n",
    "with open(output_file_path, \"w\", newline='') as output_file:\n",
    "    fieldnames = ['Asserted_Language', 'Support_Status', 'Vendor', 'File_Count', 'Predicted_Language_Count', 'Predicted_Language_Probabilities', 'District', 'State']\n",
    "    csv_writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    for asserted_language, data in combined_data.items():\n",
    "        support_status = os.path.basename(input_directory).rsplit('_', 1)[0]\n",
    "        csv_writer.writerow({\n",
    "            'Asserted_Language': asserted_language,\n",
    "            'Support_Status': support_status,\n",
    "            'Vendor': ','.join(data['vendors']),\n",
    "            'File_Count': data['file_count'],\n",
    "            'Predicted_Language_Count': ','.join([f\"{lang}:{count}\" for lang, count in data['predicted_language_count'].items()]),\n",
    "            'Predicted_Language_Probabilities': ','.join([f\"{lang}:{prob:.2f}\" for lang, prob in data['predicted_language_probabilities'].items()]),\n",
    "            'District': ','.join(data['districts']),\n",
    "            'State': ','.join(data['states'])\n",
    "        })\n",
    "\n",
    "print(f\"Combined data written to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confussion Matrix from the above csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"/data/Vaani/CSVs/Assert_lan_wise_info_supported.csv\")\n",
    "\n",
    "# Initialize a dictionary to store counts\n",
    "language_counts = {}\n",
    "\n",
    "# Iterate through the data and update counts\n",
    "for index, row in data.iterrows():\n",
    "    asserted_language = row['Asserted_Language']\n",
    "    predicted_languages = row['Predicted_Language_Count'].split(',')\n",
    "    for pred_lang in predicted_languages:\n",
    "        lang_count = pred_lang.split(':')\n",
    "        predicted_language = lang_count[0]\n",
    "        count = int(lang_count[1])\n",
    "        if asserted_language not in language_counts:\n",
    "            language_counts[asserted_language] = {}\n",
    "        language_counts[asserted_language][predicted_language] = count\n",
    "\n",
    "# Create a DataFrame from the counts dictionary\n",
    "matrix_df = pd.DataFrame(language_counts).fillna(0).astype(int)\n",
    "\n",
    "# Transpose the DataFrame twice to move asserted labels to the top and predicted labels to the left\n",
    "matrix_df = matrix_df.transpose().transpose()\n",
    "\n",
    "# Save the transposed matrix to a CSV file\n",
    "matrix_df.to_csv(\"/data/Vaani/CSVs/Asserted_vs_Predicted_Language_Counts.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize the above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"/data/Vaani/CSVs/Assert_lan_wise_info_supported.csv\")\n",
    "\n",
    "# Initialize a dictionary to store counts\n",
    "language_counts = {}\n",
    "\n",
    "# Iterate through the data and update counts\n",
    "for index, row in data.iterrows():\n",
    "    asserted_language = row['Asserted_Language']\n",
    "    predicted_languages = row['Predicted_Language_Count'].split(',')\n",
    "    for pred_lang in predicted_languages:\n",
    "        lang_count = pred_lang.split(':')\n",
    "        predicted_language = lang_count[0]\n",
    "        count = int(lang_count[1])\n",
    "        if asserted_language not in language_counts:\n",
    "            language_counts[asserted_language] = {}\n",
    "        language_counts[asserted_language][predicted_language] = count\n",
    "\n",
    "# Create a DataFrame from the counts dictionary\n",
    "matrix_df = pd.DataFrame(language_counts).fillna(0).astype(int)\n",
    "\n",
    "# Visualize the matrix as a heatmap\n",
    "plt.figure(figsize=(25, 300))\n",
    "sns.heatmap(matrix_df, cmap=\"YlGnBu\", annot=True, fmt=\"d\", linewidths=0.5)\n",
    "plt.title(\"Asserted vs Predicted Language Counts\")\n",
    "plt.xlabel(\"Asserted Language\")\n",
    "plt.ylabel(\"Predicted Language\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalize each row of the csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('/data/Vaani/CSVs/Asserted_vs_Predicted_Language_Counts_supported.csv')\n",
    "\n",
    "# Get column names except the first one (language names)\n",
    "columns = df.columns[1:]\n",
    "\n",
    "# Calculate the sum of each column\n",
    "col_sums = df[columns].sum()\n",
    "\n",
    "# Normalize each column\n",
    "normalized_df = df.copy()\n",
    "for col in columns:\n",
    "    normalized_df[col] = df[col] / col_sums[col]\n",
    "\n",
    "# Write the normalized data to a new CSV file\n",
    "normalized_df.to_csv('normalized_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# most frequent mismatch pair  pie charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read data from CSV file\n",
    "data = {}\n",
    "with open('/data/Vaani/CSVs/Asserted_vs_Predicted_Language_Counts_not_supported_normalized.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    header_languages = next(reader)[1:]\n",
    "    #print(header_languages) \n",
    "\n",
    "    for row in reader:\n",
    "        #print(row)\n",
    "        language = row[0]\n",
    "        percentage = float(row[1])\n",
    "        data[language] = percentage\n",
    "\n",
    "# Calculate percentages\n",
    "total_percentage = sum(data.values())\n",
    "threshold = 2  # Threshold percentage for inclusion in the chart\n",
    "percentages = {lang: count * 100 for lang, count in data.items() if (count * 100) >= threshold}\n",
    "other_percentage = 100 - sum(percentages.values())\n",
    "if other_percentage > 0:\n",
    "    percentages['Other'] = other_percentage\n",
    "\n",
    "# Define a color palette\n",
    "colors = plt.cm.tab20.colors  # Example color palette\n",
    "\n",
    "# Pie chart with rotated labels\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.pie(percentages.values(), labels=percentages.keys(), autopct='%1.1f%%', colors=colors, textprops={'rotation': 40, 'ha': 'right'})\n",
    "\n",
    "plt.title(f'Percentage of Times {header_languages[0]} is Predicted as Different Languages (supported)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(1,37):\n",
    "    # Read data from CSV file\n",
    "    data = {}\n",
    "    with open('/data/Vaani/CSVs/Asserted_vs_Predicted_Language_Counts_supported_normalized.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header_languages = next(reader)[1:]\n",
    "        #print(header_languages) \n",
    "\n",
    "        for row in reader:\n",
    "            #print(row)\n",
    "            language = row[0]\n",
    "            percentage = float(row[i])\n",
    "            data[language] = percentage\n",
    "\n",
    "    # Calculate percentages\n",
    "    total_percentage = sum(data.values())\n",
    "    threshold = 1.5  # Threshold percentage for inclusion in the chart\n",
    "    percentages = {lang: count * 100 for lang, count in data.items() if (count * 100) >= threshold}\n",
    "    other_percentage = 100 - sum(percentages.values())\n",
    "    if other_percentage > 0:\n",
    "        percentages['Other'] = other_percentage\n",
    "\n",
    "    # Define a color palette\n",
    "    colors = plt.cm.tab20.colors  # Example color palette\n",
    "\n",
    "    # Pie chart with rotated labels\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.pie(percentages.values(), labels=percentages.keys(), autopct='%1.1f%%', colors=colors, textprops={'rotation': 40, 'ha': 'right'})\n",
    "\n",
    "    plt.title(f'Percentage of Times {header_languages[i-1]} is Predicted as Different Languages (supported)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrive the files from csv which have totally different predicted lanuage for example tamil to hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_dir = \"/data/Vaani/CSVs/CSVs_with_link/Supported_facebook/SANTALI_predicted_labels.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_dir)\n",
    "\n",
    "# Filter rows where Detected_Language is Bengali\n",
    "#bengali_df = df[(df['Detected_Language'] == 'Hindi')].copy()\n",
    "#bengali_df = df[(df['Detected_Language'] == 'Bengali')].copy()\n",
    "#bengali_df = df[(df['Detected_Language'] == 'Kannada') | (df['Detected_Language'] == 'Telugu')].copy()\n",
    "bengali_df = df\n",
    "\n",
    "# Extract District from the File column\n",
    "bengali_df['District'] = bengali_df['File'].apply(lambda x: x.split('_')[4])\n",
    "\n",
    "# Drop the Probability and Detected_Language columns\n",
    "bengali_df = bengali_df.drop(['Probability', 'Detected_Language'], axis=1)\n",
    "\n",
    "# Add an empty column named \"Claim_Correct/Incorrect\"\n",
    "bengali_df['Claim_Correct/Incorrect'] = ''\n",
    "\n",
    "filename = \"Assert_\" + os.path.basename(csv_dir).split('_')[0].capitalize() + \".csv\"\n",
    "\n",
    "output_dir = \"/data/Vaani/CSVs/audio_files_can_be_wrong/supported/\" + filename\n",
    "\n",
    "# Write the filtered DataFrame to a new CSV file\n",
    "bengali_df.to_csv(output_dir, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaibhav_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
